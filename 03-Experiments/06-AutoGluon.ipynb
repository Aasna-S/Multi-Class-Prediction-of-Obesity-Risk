{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arham/anaconda3/envs/DataScience/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def load_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    train_df, test_df = train_test_split(df, test_size=0.35, random_state=42)\n",
    "    train_df, val_df,  = train_test_split(train_df, test_size=0.20, random_state=42)\n",
    "    train_df = train_df.drop(['id'], axis=1).drop_duplicates().reset_index(drop=True)\n",
    "    test_df = test_df.drop(['id'], axis=1).drop_duplicates().reset_index(drop=True)\n",
    "    val_df = val_df.drop(['id'], axis=1).drop_duplicates().reset_index(drop=True)\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "def encode_target(train):\n",
    "    target_key = {'Insufficient_Weight': 0, 'Normal_Weight': 1, 'Overweight_Level_I': 2, 'Overweight_Level_II': 3, 'Obesity_Type_I': 4,'Obesity_Type_II' : 5, 'Obesity_Type_III': 6}\n",
    "    train['NObeyesdad'] = train['NObeyesdad'].map(target_key)\n",
    "    return train\n",
    "\n",
    "def make_gender_binary(train):\n",
    "    train['Gender'] = train['Gender'].map({'Male':0, 'Female':1})\n",
    "\n",
    "def datatypes(train):\n",
    "    train['Weight'] = train['Weight'].astype(float)\n",
    "    train['Age'] = train['Age'].astype(float)\n",
    "    train['Height'] = train['Height'].astype(float)\n",
    "    return train\n",
    "\n",
    "def age_binning(train_df):\n",
    "    train_df['Age_Group'] = pd.cut(train_df['Age'], bins=[0, 20, 30, 40, 50, train_df['Age'].max()], labels=['0-20', '21-30', '31-40', '41-50', '50+'])\n",
    "    return train_df\n",
    "\n",
    "def age_scaling_log(train_df):\n",
    "    train_df['Age'] = train_df['Age'].astype(float)\n",
    "    train_df['Log_Age'] = np.log1p(train_df['Age'])\n",
    "    return train_df\n",
    "\n",
    "def age_scaling_minmax(train_df):\n",
    "    train_df['Age'] = train_df['Age'].astype(float)\n",
    "    scaler_age = MinMaxScaler()\n",
    "    train_df['Scaled_Age'] = scaler_age.fit_transform(train_df['Age'].values.reshape(-1, 1))\n",
    "    return train_df, scaler_age\n",
    "\n",
    "def weight_scaling_log(train_df):\n",
    "    train_df['Weight'] = train_df['Weight'].astype(float)\n",
    "    train_df['Log_Weight'] = np.log1p(train_df['Weight'])\n",
    "    return train_df\n",
    "\n",
    "def weight_scaling_minmax(train_df):\n",
    "    train_df['Weight'] = train_df['Weight'].astype(float)\n",
    "    scaler_weight = MinMaxScaler()\n",
    "    train_df['Scaled_Weight'] = scaler_weight.fit_transform(train_df['Weight'].values.reshape(-1, 1))\n",
    "    return train_df, scaler_weight\n",
    "\n",
    "def height_scaling_log(train_df):\n",
    "    train_df['Log_Height'] = np.log1p(train_df['Height'])\n",
    "    return train_df\n",
    "\n",
    "def height_scaling_minmax(train_df):\n",
    "    scaler_height = MinMaxScaler()\n",
    "    train_df['Scaled_Height'] = scaler_height.fit_transform(train_df['Height'].values.reshape(-1, 1))\n",
    "    return train_df, scaler_height\n",
    "\n",
    "def make_gender_binary(train):\n",
    "    train['Gender'] = train['Gender'].map({'Female':1, 'Male':0})\n",
    "    return train\n",
    "\n",
    "def fix_binary_columns(train):\n",
    "    Binary_Cols = ['family_history_with_overweight','FAVC', 'SCC','SMOKE']\n",
    "    # if yes then 1 else 0\n",
    "    for col in Binary_Cols:\n",
    "        train[col] = train[col].map({'yes': 1, 'no': 0})\n",
    "    return train\n",
    "\n",
    "def freq_cat_cols(train):\n",
    "    # One hot encoding\n",
    "    cat_cols = ['CAEC', 'CALC']\n",
    "    for col in cat_cols:\n",
    "        train[col] = train[col].map({'no': 0, 'Sometimes': 1, 'Frequently': 2, 'Always': 3})\n",
    "    return train\n",
    "\n",
    "def Mtrans(train):\n",
    "    \"\"\"\n",
    "    Public_Transportation    8692\n",
    "    Automobile               1835\n",
    "    Walking                   231\n",
    "    Motorbike                  19\n",
    "    Bike                       16\n",
    "    \"\"\"\n",
    "    # train['MTRANS'] = train['MTRANS'].map({'Public_Transportation': 3, 'Automobile': 5, 'Walking': 1, 'Motorbike': 4, 'Bike': 2})\n",
    "    # dummify column\n",
    "    train = pd.get_dummies(train, columns=['MTRANS'])\n",
    "    return train\n",
    "\n",
    "\n",
    "def other_features(train):\n",
    "    train['BMI'] = train['Weight'] / (train['Height'] ** 2)\n",
    "    # train['Age'*'Gender'] = train['Age'] * train['Gender']\n",
    "    polynomial_features = PolynomialFeatures(degree=2)\n",
    "    X_poly = polynomial_features.fit_transform(train[['Age', 'BMI']])\n",
    "    poly_features_df = pd.DataFrame(X_poly, columns=['Age^2', 'Age^3', 'BMI^2', 'Age * BMI', 'Age * BMI^2', 'Age^2 * BMI^2'])\n",
    "    train = pd.concat([train, poly_features_df], axis=1)\n",
    "    return train\n",
    "\n",
    "\n",
    "def test_pipeline(test, scaler_age, scaler_weight, scaler_height):\n",
    "    test = datatypes(test)\n",
    "    test = encode_target(test)\n",
    "    test = age_binning(test)\n",
    "    test = age_scaling_log(test)\n",
    "    test['Scaled_Age'] = scaler_age.transform(test['Age'].values.reshape(-1, 1))\n",
    "    test = weight_scaling_log(test)\n",
    "    test['Scaled_Weight'] = scaler_weight.transform(test['Weight'].values.reshape(-1, 1))\n",
    "    test = height_scaling_log(test)\n",
    "    test['Scaled_Height'] = scaler_height.transform(test['Height'].values.reshape(-1, 1))\n",
    "    test = make_gender_binary(test)\n",
    "    test = fix_binary_columns(test)\n",
    "    test = freq_cat_cols(test)\n",
    "    test = Mtrans(test)\n",
    "    test = other_features(test)\n",
    "\n",
    "    return test\n",
    "\n",
    "def train_model(params, X_train, y_train):\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "    model = lgb.train(params, lgb_train, num_boost_round=1000)\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X_val, y_val):\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred = [np.argmax(y) for y in y_pred]\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "def objective(trial, X_train, y_train):\n",
    "    params = {\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': 7,\n",
    "        'metric': 'multi_logloss',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', -1, 20),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.6, 0.95),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.6, 0.95),\n",
    "        'verbosity': -1\n",
    "    }\n",
    "\n",
    "    n_splits = 5\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    for train_index, val_index in kf.split(X_train, y_train):\n",
    "        X_tr, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_tr, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "        model = train_model(params, X_tr, y_tr)\n",
    "        accuracy = evaluate_model(model, X_val, y_val)\n",
    "        scores.append(accuracy)\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "def optimize_hyperparameters(X_train, y_train, n_trials=2):\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=n_trials)\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/arham/Downloads/Projects/01-Dataset/01-Data-for-model-building/train.csv'\n",
    "train_df, val_df, test_df = load_data(path)\n",
    "\n",
    "train_df = datatypes(train_df)\n",
    "train_df = encode_target(train_df)\n",
    "train_df = age_binning(train_df)\n",
    "train_df, scaler_age = age_scaling_minmax(train_df)\n",
    "train_df = age_scaling_log(train_df)\n",
    "train_df, scaler_weight = weight_scaling_minmax(train_df)\n",
    "train_df = weight_scaling_log(train_df)\n",
    "train_df, scaler_height = height_scaling_minmax(train_df)\n",
    "train_df = height_scaling_log(train_df)\n",
    "train_df = make_gender_binary(train_df)\n",
    "train_df = fix_binary_columns(train_df)\n",
    "train_df = freq_cat_cols(train_df)\n",
    "train_df = Mtrans(train_df)\n",
    "train_df = other_features(train_df)\n",
    "\n",
    "val_df = test_pipeline(val_df, scaler_age, scaler_weight, scaler_height)\n",
    "test_df = test_pipeline(test_df, scaler_age, scaler_weight, scaler_height)\n",
    "\n",
    "Target = 'NObeyesdad'\n",
    "features = train_df.columns.drop(Target)\n",
    "\n",
    "features = ['Gender', 'Age', 'Height', 'Weight', 'family_history_with_overweight',\n",
    "       'FAVC', 'FCVC', 'NCP', 'CAEC', 'SMOKE', 'CH2O', 'SCC', 'FAF', 'TUE',\n",
    "       'CALC', 'Age_Group', \n",
    "       'MTRANS_Automobile', 'MTRANS_Bike', 'MTRANS_Motorbike',\n",
    "       'MTRANS_Public_Transportation', 'MTRANS_Walking', 'BMI', 'Age^2',\n",
    "       'Age^3', 'BMI^2', 'Age * BMI', 'Age * BMI^2', 'Age^2 * BMI^2'] \n",
    "#'Scaled_Age', 'Log_Age', 'Scaled_Weight', 'Log_Weight', 'Scaled_Height', 'Log_Height',\n",
    "\n",
    "X_train = train_df[features]\n",
    "y_train = train_df[Target]\n",
    "X_val = val_df[features]\n",
    "y_val = val_df[Target]\n",
    "X_test = test_df[features]\n",
    "y_test = test_df[Target]\n",
    "\n",
    "#combine X_train and y_train as one dataframe\n",
    "tr = pd.concat([X_train, y_train], axis=1)\n",
    "te = pd.concat([X_test, y_test], axis =1)\n",
    "va = pd.concat([X_val, y_val], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240421_232856\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240421_232856\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 23.0.0: Fri Sep 15 14:42:57 PDT 2023; root:xnu-10002.1.13~1/RELEASE_ARM64_T8112\n",
      "CPU Count:          8\n",
      "Memory Avail:       1.18 GB / 8.00 GB (14.8%)\n",
      "Disk Space Avail:   4.92 GB / 228.27 GB (2.2%)\n",
      "\tWARNING: Available disk space is low and there is a risk that AutoGluon will run out of disk during fit, causing an exception. \n",
      "\tWe recommend a minimum available disk space of 10 GB, and large datasets may require more.\n",
      "===================================================\n",
      "Train Data Rows:    10793\n",
      "Train Data Columns: 28\n",
      "Tuning Data Rows:    2699\n",
      "Tuning Data Columns: 28\n",
      "Label Column:       NObeyesdad\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
      "\t7 unique label values:  [0, 3, 1, 6, 4, 2, 5]\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 7\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1207.76 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2.34 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 10 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['Age^2']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 2): ['Age^3', 'BMI^2']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 2 | ['Age^3', 'BMI^2']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])     :  5 | ['MTRANS_Automobile', 'MTRANS_Bike', 'MTRANS_Motorbike', 'MTRANS_Public_Transportation', 'MTRANS_Walking']\n",
      "\t\t('category', []) :  1 | ['Age_Group']\n",
      "\t\t('float', [])    : 12 | ['Age', 'Height', 'Weight', 'FCVC', 'NCP', ...]\n",
      "\t\t('int', [])      :  7 | ['Gender', 'family_history_with_overweight', 'FAVC', 'CAEC', 'SMOKE', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  :  1 | ['Age_Group']\n",
      "\t\t('float', [])     : 12 | ['Age', 'Height', 'Weight', 'FCVC', 'NCP', ...]\n",
      "\t\t('int', [])       :  2 | ['CAEC', 'CALC']\n",
      "\t\t('int', ['bool']) : 10 | ['Gender', 'family_history_with_overweight', 'FAVC', 'SMOKE', 'SCC', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t25 features in original data used to generate 25 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1.58 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.36s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.8318\t = Validation score   (accuracy)\n",
      "\t2.51s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.8281\t = Validation score   (accuracy)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t0.8725\t = Validation score   (accuracy)\n",
      "\t9.48s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\t0.8963\t = Validation score   (accuracy)\n",
      "\t35.57s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t0.9037\t = Validation score   (accuracy)\n",
      "\t47.8s\t = Training   runtime\n",
      "\t0.24s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.8929\t = Validation score   (accuracy)\n",
      "\t1.65s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.8922\t = Validation score   (accuracy)\n",
      "\t1.87s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t0.9011\t = Validation score   (accuracy)\n",
      "\t35.88s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.8896\t = Validation score   (accuracy)\n",
      "\t0.9s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.8914\t = Validation score   (accuracy)\n",
      "\t0.87s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.907\t = Validation score   (accuracy)\n",
      "\t18.63s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\tmodule 'torch.utils._pytree' has no attribute 'register_pytree_node'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/arham/anaconda3/envs/DataScience/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/Users/arham/anaconda3/envs/DataScience/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/Users/arham/anaconda3/envs/DataScience/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 855, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/Users/arham/anaconda3/envs/DataScience/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 196, in _fit\n",
      "    self.optimizer = self._init_optimizer(**optimizer_kwargs)\n",
      "  File \"/Users/arham/anaconda3/envs/DataScience/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 553, in _init_optimizer\n",
      "    optimizer = torch.optim.Adam(params=self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
      "  File \"/Users/arham/anaconda3/envs/DataScience/lib/python3.10/site-packages/torch/optim/adam.py\", line 45, in __init__\n",
      "    super().__init__(params, defaults)\n",
      "  File \"/Users/arham/anaconda3/envs/DataScience/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 266, in __init__\n",
      "    self.add_param_group(cast(dict, param_group))\n",
      "  File \"/Users/arham/anaconda3/envs/DataScience/lib/python3.10/site-packages/torch/_compile.py\", line 22, in inner\n",
      "    import torch._dynamo\n",
      "  File \"/Users/arham/anaconda3/envs/DataScience/lib/python3.10/site-packages/torch/_dynamo/__init__.py\", line 2, in <module>\n",
      "    from . import allowed_functions, convert_frame, eval_frame, resume_execution\n",
      "  File \"/Users/arham/anaconda3/envs/DataScience/lib/python3.10/site-packages/torch/_dynamo/allowed_functions.py\", line 26, in <module>\n",
      "    from . import config\n",
      "  File \"/Users/arham/anaconda3/envs/DataScience/lib/python3.10/site-packages/torch/_dynamo/config.py\", line 49, in <module>\n",
      "    torch.onnx.is_in_onnx_export: False,\n",
      "  File \"/Users/arham/anaconda3/envs/DataScience/lib/python3.10/site-packages/torch/__init__.py\", line 1831, in __getattr__\n",
      "    return importlib.import_module(f\".{name}\", __name__)\n",
      "  File \"/Users/arham/anaconda3/envs/DataScience/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/Users/arham/anaconda3/envs/DataScience/lib/python3.10/site-packages/torch/onnx/__init__.py\", line 46, in <module>\n",
      "    from ._internal.exporter import (  # usort:skip. needs to be last to avoid circular import\n",
      "  File \"/Users/arham/anaconda3/envs/DataScience/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py\", line 42, in <module>\n",
      "    from torch.onnx._internal.fx import (\n",
      "  File \"/Users/arham/anaconda3/envs/DataScience/lib/python3.10/site-packages/torch/onnx/_internal/fx/__init__.py\", line 1, in <module>\n",
      "    from .patcher import ONNXTorchPatcher\n",
      "  File \"/Users/arham/anaconda3/envs/DataScience/lib/python3.10/site-packages/torch/onnx/_internal/fx/patcher.py\", line 11, in <module>\n",
      "    import transformers  # type: ignore[import]\n",
      "  File \"/Users/arham/anaconda3/envs/DataScience/lib/python3.10/site-packages/transformers/__init__.py\", line 26, in <module>\n",
      "    from . import dependency_versions_check\n",
      "  File \"/Users/arham/anaconda3/envs/DataScience/lib/python3.10/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
      "    from .utils.versions import require_version, require_version_core\n",
      "  File \"/Users/arham/anaconda3/envs/DataScience/lib/python3.10/site-packages/transformers/utils/__init__.py\", line 33, in <module>\n",
      "    from .generic import (\n",
      "  File \"/Users/arham/anaconda3/envs/DataScience/lib/python3.10/site-packages/transformers/utils/generic.py\", line 455, in <module>\n",
      "    _torch_pytree.register_pytree_node(\n",
      "AttributeError: module 'torch.utils._pytree' has no attribute 'register_pytree_node'. Did you mean: '_register_pytree_node'?\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t0.9014\t = Validation score   (accuracy)\n",
      "\t83.26s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'XGBoost': 0.45, 'RandomForestEntr': 0.2, 'CatBoost': 0.1, 'ExtraTreesGini': 0.1, 'LightGBMXT': 0.05, 'LightGBM': 0.05, 'RandomForestGini': 0.05}\n",
      "\t0.9107\t = Validation score   (accuracy)\n",
      "\t0.25s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 244.91s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240421_232856\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9107076695072249, 'balanced_accuracy': 0.903291645825612, 'mcc': 0.8955040990427827}\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "# Train AutoGluon model\n",
    "predictor = TabularPredictor(label=Target).fit(train_data=tr, tuning_data=va)\n",
    "\n",
    "# Evaluate on validation data\n",
    "performance = predictor.evaluate(va)\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred = predictor.predict(te)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(performance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving probabilities for stacked model later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on train, validation, and test data\n",
    "y_pred_train = predictor.predict(tr)\n",
    "y_pred_va = predictor.predict(va)\n",
    "y_pred_test = predictor.predict(te)\n",
    "\n",
    "# Get prediction probabilities for each class\n",
    "y_probabilities_train = predictor.predict_proba(tr)\n",
    "y_probabilities_va = predictor.predict_proba(va)\n",
    "y_probabilities_test = predictor.predict_proba(te)\n",
    "\n",
    "# add to tr, te, va\n",
    "test = pd.concat([te, y_probabilities_test.iloc[:, -7:]], axis=1)\n",
    "train = pd.concat([tr, y_probabilities_train.iloc[:, -7:]], axis=1)\n",
    "val = pd.concat([va, y_probabilities_va.iloc[:, -7:]], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>family_history_with_overweight</th>\n",
       "      <th>FAVC</th>\n",
       "      <th>FCVC</th>\n",
       "      <th>NCP</th>\n",
       "      <th>CAEC</th>\n",
       "      <th>SMOKE</th>\n",
       "      <th>...</th>\n",
       "      <th>Age * BMI^2</th>\n",
       "      <th>Age^2 * BMI^2</th>\n",
       "      <th>NObeyesdad</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1.550000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>445.785640</td>\n",
       "      <td>450.623213</td>\n",
       "      <td>0</td>\n",
       "      <td>0.797175</td>\n",
       "      <td>0.196596</td>\n",
       "      <td>0.005617</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>553.633218</td>\n",
       "      <td>766.274350</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.001926</td>\n",
       "      <td>0.047782</td>\n",
       "      <td>0.948167</td>\n",
       "      <td>0.002041</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>421.875000</td>\n",
       "      <td>549.316406</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011628</td>\n",
       "      <td>0.953612</td>\n",
       "      <td>0.033047</td>\n",
       "      <td>0.001538</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>1.632983</td>\n",
       "      <td>111.720238</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1089.285877</td>\n",
       "      <td>1755.242193</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.999759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>21.682636</td>\n",
       "      <td>1.748524</td>\n",
       "      <td>133.845064</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>949.229536</td>\n",
       "      <td>1916.541944</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.999831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10788</th>\n",
       "      <td>0</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.780000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>613.558894</td>\n",
       "      <td>1161.896656</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.998704</td>\n",
       "      <td>0.000949</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10789</th>\n",
       "      <td>1</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>1.641601</td>\n",
       "      <td>111.830924</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1078.946835</td>\n",
       "      <td>1722.080284</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.999757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10790</th>\n",
       "      <td>0</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1.770000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>502.729101</td>\n",
       "      <td>573.098750</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.341897</td>\n",
       "      <td>0.657261</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10791</th>\n",
       "      <td>0</td>\n",
       "      <td>29.669219</td>\n",
       "      <td>1.774644</td>\n",
       "      <td>105.966894</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.934671</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>998.283353</td>\n",
       "      <td>1132.127734</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.004232</td>\n",
       "      <td>0.936736</td>\n",
       "      <td>0.057409</td>\n",
       "      <td>0.000597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10792</th>\n",
       "      <td>1</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>1.656504</td>\n",
       "      <td>111.884535</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1060.128308</td>\n",
       "      <td>1662.532588</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.999719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10793 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Gender        Age    Height      Weight  \\\n",
       "0           1  21.000000  1.550000   51.000000   \n",
       "1           0  20.000000  1.700000   80.000000   \n",
       "2           1  18.000000  1.600000   60.000000   \n",
       "3           1  26.000000  1.632983  111.720238   \n",
       "4           1  21.682636  1.748524  133.845064   \n",
       "...       ...        ...       ...         ...   \n",
       "10788       0  18.000000  1.780000  108.000000   \n",
       "10789       1  26.000000  1.641601  111.830924   \n",
       "10790       0  21.000000  1.770000   75.000000   \n",
       "10791       0  29.669219  1.774644  105.966894   \n",
       "10792       1  26.000000  1.656504  111.884535   \n",
       "\n",
       "       family_history_with_overweight  FAVC      FCVC  NCP  CAEC  SMOKE  ...  \\\n",
       "0                                   0     1  3.000000  1.0     2      0  ...   \n",
       "1                                   1     1  2.000000  3.0     1      0  ...   \n",
       "2                                   1     1  2.000000  3.0     1      0  ...   \n",
       "3                                   1     1  3.000000  3.0     1      0  ...   \n",
       "4                                   1     1  3.000000  3.0     1      0  ...   \n",
       "...                               ...   ...       ...  ...   ...    ...  ...   \n",
       "10788                               1     1  2.000000  3.0     1      0  ...   \n",
       "10789                               1     1  3.000000  3.0     1      0  ...   \n",
       "10790                               0     1  3.000000  3.0     2      0  ...   \n",
       "10791                               1     1  2.934671  3.0     1      0  ...   \n",
       "10792                               1     1  3.000000  3.0     1      0  ...   \n",
       "\n",
       "       Age * BMI^2  Age^2 * BMI^2  NObeyesdad         0         1         2  \\\n",
       "0       445.785640     450.623213           0  0.797175  0.196596  0.005617   \n",
       "1       553.633218     766.274350           3  0.000041  0.001926  0.047782   \n",
       "2       421.875000     549.316406           1  0.011628  0.953612  0.033047   \n",
       "3      1089.285877    1755.242193           6  0.000001  0.000002  0.000008   \n",
       "4       949.229536    1916.541944           6  0.000001  0.000001  0.000004   \n",
       "...            ...            ...         ...       ...       ...       ...   \n",
       "10788   613.558894    1161.896656           4  0.000007  0.000014  0.000035   \n",
       "10789  1078.946835    1722.080284           6  0.000001  0.000002  0.000007   \n",
       "10790   502.729101     573.098750           2  0.000212  0.341897  0.657261   \n",
       "10791   998.283353    1132.127734           4  0.000621  0.000067  0.000338   \n",
       "10792  1060.128308    1662.532588           6  0.000001  0.000002  0.000007   \n",
       "\n",
       "              3         4         5         6  \n",
       "0      0.000520  0.000032  0.000019  0.000040  \n",
       "1      0.948167  0.002041  0.000018  0.000025  \n",
       "2      0.001538  0.000150  0.000005  0.000020  \n",
       "3      0.000018  0.000172  0.000040  0.999759  \n",
       "4      0.000012  0.000092  0.000058  0.999831  \n",
       "...         ...       ...       ...       ...  \n",
       "10788  0.000287  0.998704  0.000949  0.000005  \n",
       "10789  0.000020  0.000154  0.000059  0.999757  \n",
       "10790  0.000391  0.000052  0.000019  0.000168  \n",
       "10791  0.004232  0.936736  0.057409  0.000597  \n",
       "10792  0.000021  0.000173  0.000076  0.999719  \n",
       "\n",
       "[10793 rows x 36 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
